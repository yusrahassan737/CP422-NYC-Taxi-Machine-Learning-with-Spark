{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2558cb77-bdbe-41ed-b17f-d0f35ff897ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h1>Assignment 3</h1>\n",
    "<h2>Analyzing NYC Taxi Trip Data with Databricks and Apache Spark</h2>\n",
    "<h4>By Group 10 (Faiza, Wardah, Amany, Yusra, Sara, Anna), Due 2025-11-26</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af15bb34-f6d9-4df1-bc20-ebfb780304f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h3>Task 1: Classification</h3>\n",
    "<h4>Objective: Build a classification pipeline to predict whether a trip resulted in a high fare (e.g., over $20) based on trip characteristics.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb3bf4e-3fdd-4fca-ace3-6481e42e1412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h4>Step 1: Data Preparation</h4>\n",
    "\n",
    "a) Load Dataset: Load the CSV file into a Spark DataFrame.</ol>\n",
    "\n",
    "b) Data Exploration: Explore the dataset to understand the variables and handle missing or invalid values. </ol>\n",
    "\n",
    "c) Feature Engineering: Create new features or modify existing ones to improve the model. For example:\n",
    "- Time-based features (e.g., hour, day of the week) from the pickup or drop-off times.\n",
    "- Distance calculation between pickup and drop-off coordinates.\n",
    "\n",
    "d) Target Variable Creation: Define a binary target column high_fare where:\n",
    "- 1 if the fare is above $20.\n",
    "- 0 if the fare is $20 or below.\n",
    "\n",
    "e) Data Splitting: Split the data into training (70%) and testing (30%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec236cf5-cfe0-48a4-b214-0dbfde452611",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, unix_timestamp, col, mean, max\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64fbfb1c-e471-410a-a6e0-14bc3a78a3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# a) Load the data, fix types\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/Volumes/workspace/default/assignment/yellow_tripdata_2015-01.csv\")\n",
    "df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"int\"))\n",
    "df = df.withColumn(\"trip_distance\", col(\"trip_distance\").cast(\"float\"))\n",
    "df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54234022-e196-4700-8312-a2fb7bc0ecc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|tpep_pickup_datetime|\n+--------------------+\n| 2015-01-21 05:45:18|\n| 2015-01-06 11:49:54|\n| 2015-01-10 21:36:52|\n| 2015-01-25 00:13:08|\n| 2015-01-30 19:50:04|\n| 2015-01-10 21:13:52|\n| 2015-01-01 22:47:45|\n| 2015-01-10 19:12:25|\n| 2015-01-10 22:53:25|\n| 2015-01-20 15:51:26|\n| 2015-01-15 16:15:15|\n| 2015-01-09 20:29:42|\n| 2015-01-10 21:36:51|\n| 2015-01-23 16:51:38|\n| 2015-01-04 13:44:51|\n| 2015-01-29 21:33:55|\n| 2015-01-21 07:27:00|\n| 2015-01-10 21:27:46|\n| 2015-01-27 16:22:03|\n| 2015-01-25 17:45:13|\n+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# b) Data Exploration\n",
    "# Handle missing values by removing them for columns “fare_amount, trip_distance, and passenger_count.”\n",
    "df1 = df.dropna(subset = [\"fare_amount\", \"trip_distance\", \"passenger_count\"])\n",
    "# Filter out rows with invalid data (e.g., fare_amount < 0 or trip_distance = 0)\n",
    "df1 = df1.filter((df1.fare_amount > 0) & (df1.trip_distance != 0))\n",
    "# Convert the pickup_datetime and dropoff_datetime columns to timestamp data types\n",
    "df1 = df1.withColumn(\n",
    "    \"tpep_pickup_datetime\",\n",
    "    to_timestamp(\"tpep_pickup_datetime\"))\n",
    "df1 = df1.withColumn(\n",
    "    \"tpep_dropoff_datetime\",\n",
    "    to_timestamp(\"tpep_dropoff_datetime\"))\n",
    "df1.select(\"tpep_pickup_datetime\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9ff0469-e32e-481f-8b50-772887911ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<h4>Step 2: Decision Tree Classifier Pipeline</h4>\n",
    "\n",
    "a) Define the pipeline stages:\n",
    "\n",
    "- Feature Transformers (VectorAssembler, StandardScaler, etc.).\n",
    "\n",
    "- Model: Decision Tree Classifier.\n",
    "\n",
    "b) Hyperparameter Tuning: Use CrossValidator with GridSearch to find the best parameters (e.g., max depth, min instances per node).\n",
    "\n",
    "c) Model Training: Train the pipeline on the training data.\n",
    "\n",
    "d) Model Evaluation: Evaluate performance on the test data using metrics like F1 Score, Precision, and Recall.\n",
    "\n",
    "e) Save Pipeline: Save the trained pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1da3d9ac-6477-4334-a7f8-f69eeed8619c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h4>Step 3: Logistic Regression Pipeline</h4>\n",
    "\n",
    "a) Define a new pipeline with Logistic Regression as the classifier.\n",
    "\n",
    "b) Perform hyperparameter tuning on Logistic Regression parameters (e.g., regularization parameter, max iterations).\n",
    "\n",
    "c) Evaluate model performance on test data and compare with the Decision Tree Classifier pipeline.\n",
    "\n",
    "d) Save the trained pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b73bc049-1a4c-41da-958f-b3d4cbd1174f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h4>Step 4: Report Findings</h4>\n",
    "\n",
    "a) Discuss the performance of each pipeline and which hyperparameters were chosen.\n",
    "\n",
    "b) State the best-performing pipeline and explain why it performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "818dfe89-671c-4a1e-9325-12c92e1d646e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h3>Task 2: Regression</h3>\n",
    "<h4>Objective: Build a regression pipeline to predict the fare amount based on trip characteristics.</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54a89977-a466-4b6d-a563-14dde383ead3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "\n",
    "<h4>Step 1: Data Preparation</h4>\n",
    "\n",
    "a) Load Dataset: Use the same dataset and load it into a new DataFrame.</ol>\n",
    "\n",
    "b) Feature Engineering: Similar to the classification task, but focus on features that might help predict fare amount.\n",
    "\n",
    "Time-based features (pickup hour, day of week).\n",
    "\n",
    "Trip distance and trip duration.\n",
    "\n",
    "c) Data Splitting: Split the data into training (70%) and testing (30%) sets.</ol>\n",
    "\n",
    "<h4>Step 2: Linear Regression Pipeline</h4>\n",
    "\n",
    "a) Define the pipeline stages:\n",
    "\n",
    "Feature Transformers (VectorAssembler, StandardScaler, etc.).\n",
    "\n",
    "Model: Linear Regression.\n",
    "\n",
    "b) Hyperparameter Tuning: Use CrossValidator with GridSearch to tune hyperparameters (e.g., regularization parameter, max iterations).\n",
    "\n",
    "c) Model Training: Train the pipeline on the training data.\n",
    "\n",
    "d) Model Evaluation: Evaluate the model using RMSE (Root Mean Squared Error) and R² Score on the test data.\n",
    "\n",
    "e) Save Pipeline: Save the trained pipeline.\n",
    "\n",
    "<h4>Step 3: Random Forest Regressor Pipeline</h4>\n",
    "\n",
    "a) Define a new pipeline with Random Forest Regressor.\n",
    "\n",
    "b) Perform hyperparameter tuning on Random Forest parameters (e.g., number of trees, max depth).\n",
    "\n",
    "c) Evaluate the model performance on test data and compare it with the Linear Regression pipeline.\n",
    "\n",
    "d) Save the trained pipeline.\n",
    "\n",
    "<h4>Step 4: Report Findings</h4>\n",
    "\n",
    "a) Discuss the performance of each pipeline and provide a comparison.\n",
    "\n",
    "b) Justify which pipeline you would choose for future predictions based on the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe380ae-d1aa-446c-9acd-0afff2c75a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Assignment 3 -Group 10",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}